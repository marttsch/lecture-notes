\documentclass[mfit.tex]{subfiles}
\begin{document}

\lecture{09.03.2016}

Joint distributions/marginal distributions\\
Independencs of RVs

multdim RV:

\[(X_1,\dots,X_n): (\Omega,\mathcal{A},\prob) \to (\R^n, \mathcal{B}_{\R^n}) \]
\todo{add pic}
\[ [(X_1,\dots,X_n) \in B] = \inv{(X_1,\dots,X_n)}(B) \in \mathcal{A} \]

\begin{align} \label{star1}
  P_{X_1,\dots,X_n}(B) = \prob[(X_1,\dots,X_n) \in B]
\end{align}
is a probability measure on $\R^n$: joint distribution of $X_1,\dots,X_n$. \\
$P_{X_k}$ probability on $\R$. It is called the $k$-th marginal of $P_{X_1,\dots,X_k}$
\todo{add pic}

$P_{X_k}(I) = \prob[X_k \in I] = \prob[(X_1,\dots,X_k) \in \R \times \R \times \dots \times \underbrace{I}_{k} \times R \times \R]$ where $I \subset \R$ interval.\\
\\
discrete:\\
$(X_1,\dots,X_n)$ takes at most countable many values in $\R^d$.\\
joint discrete density: $p_{X_1,\dots,X_n}(x_1,\dots,x_n) = \prob[X_1=x_1,\dots,X_n=x_n]$
\begin{align*}
  \sum_{(x_1,\dots,x_n)} p_n(x_1,\dots,x_n) = t \\
  \prob[(X_1,\dots,X_n) \in B] = \sum_{(x_1,\dots,x_n) \in B} p_n(x_1,\dots,x_n)
\end{align*}

\begin{align*}
  p_{X_k}(a) = \sum_{(x_1,\dots,x_n) \in \R^n, x_k = a} p_n(x_1,\dots,x_n) 
\end{align*}

often two RVs: $(X,Y)$. Then
\[ p_{X,Y} (x,y) = \prob[X = x, Y = y] \]
\[ p_Y(y) = \sum_{x \in \R} p_{X,Y} (x,y) \]

continuous: similar
\begin{align*}
  \ref{star1} = {\int \dots \int}_B \underbrace{f_{X_1,\dots,X_n} (x_1,\dots,x_n)}_{\text{joint density}} dx_1\dots dx_n
\end{align*}
$k$-th marginal:
\begin{align*}
  f_{X_k}(x_k) = 
\end{align*} \todo{add pic}

\begin{defi*}
  RVs $X_1,\dots,X_n$ are called independent if for all intervals (Borel sets in $\R$) $I_1,\dots,I_n \subset \R$
  \[ [X_1 \in I_1],\dots,[X_n \in I_n]\]
  are independent.
  Then 
  \begin{align*}
    \prob[X_1 \in I_1, X_2 \in I_2,\dots,X_n \in I_n] = \prob[X_1 \in I_1] \dots \prob[X_n \in I_n]
  \end{align*}
\end{defi*}

for discrete/continuous cases:
\begin{align*}
  p_{X_1,\dots,X_n} (x_1,\dots,x_n) = p_{X_1}(x_1) \dots p_{X_n}(x_n)
\end{align*}
The same holds for $f$ almost everywhere.

\section{Variance: Markov- and Chebyshev inequalities}

Recall: 
\begin{align*}
  \expect(X) = \int_\Omega X d\prob = \begin{cases} \sum_x xp(x)\\ \int_\R x f(x) dx \end{cases}
\end{align*}

\begin{defi*}
  Let $X$ be a random variable such that $\expect(X)$ exists and is finite.
  Then the variance is defined as
  \[ \vari(X) \coloneqq \expect((X-\expect(X))^2) \]
\end{defi*}

\todo{add pic}

Recall: If $X_1,X_2$ are RVs with finite expectation, then
\[ \expect( \lambda_1 X_1 + \lambda_2 X_2) = \lambda_1 \expect(X_1) + \lambda_2 \expect(X_2) \]
If $X_1 \geq 0$ a.s., then $\expect(X_1) \geq 0$.
Now $X$ RV:
\[ g: (\R,\mathcal{B}) \to (\R,\mathcal{B}) \]
measurable (good, e.g. continuous)
\begin{align*}
  g(X) (\omega) = g \circ X(\omega) = g(X(\omega)) 
\end{align*}

\begin{ex}
  $\expect((X-\expect(X))^2)$, then $g(x) = (x - \mu)^2$.
\end{ex}

If $\expect(g(X))$ is finite, then
\[ = \int_\R g(x) dP_X(x) \]
\begin{align*}
  = \begin{cases} \sum_{x \in \R} g(x) p_X(x) & \text{ discrete}\\ \int_\R g(x) f_X(x) dx & \text{ continuous} \end{cases}
\end{align*}

Now consider:
\begin{align*}
  (X-\expect(X))^2 &= X^2 - 2 \expect(X) X + \expect(X)^2 \\
  &= X^2 - 2 \mu X + \mu^2 \\
  \vari(X) &= \expect(X^2) - 2\mu \expect(X) + \mu^2 \\
  &= \expect(X^2) - 2 \mu \mu + \mu^2 \\
  &= \expect(X^2) - \expect(X)^2
\end{align*}

Let $(X,Y)$ be a $2$-dim RV with $\expect(X)$ and $\expect(Y)$ finite. Then we know
\[ \expect(X+Y) = \expect(X) + \expect(Y) \].
Consider now the variance:
\begin{align*}
  \vari(X+Y) &= \expect((X+Y - \expect(X) -\expect(Y))^2) \\
  &= \expect((X-\expect(X))^2) + \expect((Y-\expect(Y))^2) + 2 \expect((X-\expect(X))(Y-\expect(Y))) \\
  &= \vari(X) + \vari(Y) + 2 \covar(X,Y)
\end{align*}
Important fact: if $X,Y$ are independent (and the involved expected values are finite), then
\[ \expect(X \cdot Y) = \expect(X) \expect(Y) \]
\\
\\
discrete case:\\
$(X,Y): (\Omega,\mathcal{A},\prob) \to (\R^2,\mathcal{B}_{\R^2})$
\begin{align*}
  g: \R^2 &\to \R \\
  g(x,y) &= x \cdot y
\end{align*}
Then
\begin{align*}
  \expect(g(X,Y)) &= \sum_{(x,y) \in \R^2} g(x,y) \cdot p_{X,Y} (x,y) \\
  &= \sum_{x} \sum_{y} xy p_X(x) p_Y(y) \\
  &= \sum_x x p_X(x) \sum_y y p_Y(y) \\
  &= \expect(X) \expect(Y)
\end{align*}

\emph{Formula} in independent case:
\[ \vari(X+Y) = \vari(X) + \vari(Y) \]
\[ \covar(X,Y) = \expect((X-\expect(X)) (Y - \expect(Y)) \]
\[ \expect(X-\expect(X)) \expect(Y-\expect(Y)) = 0 \]
Trap for exam:
\[ \vari(X-Y) = \vari(X) + \vari(-Y)  = \vari(X) + \vari(Y) \]

\begin{lemma}[Markov inequality]
  Let $X$ be a non-negative real random variable. And $0 < \expect(X) < \infty$.
  Then
  \[ \prob[X \geq a \expect(X)] \leq \frac{1}{a} \]
  for all $a > 0$.
\end{lemma}

\begin{proof}
  Let $I = \mathbb{1}_{[X \geq a \expect(X)]}$ be a random variable.
  $I(\omega) = \begin{cases} 1 & X(\omega) \geq a \expect(X)\\ 0 & \text{otherwise} \end{cases}$
  \[ \expect(I) = \prob[X \geq a \expect(X)] \]
  \[ X \geq X \cdot I \geq a \expect(X) \cdot I \]
  \[ \expect(X) \geq \expect(a \expect(X) I) = a\expect(X) \cdot \expect(I) = a \expect(X) \prob[X \geq a \expect(X)] \]
  Then
  \[ \frac{1}{a} \geq \prob[X \geq a \expect(X)] \]
  \begin{align*}
    j
  \end{align*}
\end{proof}

\begin{rem}[Exercise]
  Let $X \geq 0$ a.s. and $\expect(X) = 0$. Then $X = 0$ a.s.
\end{rem}

\begin{cor}[Chebyshev inequality]
  Let $X$ be an arbitrary random variable such that $\expect(X)$, $\vari(X)$ are finite.
  Then for any $a > 0$
  \[ \prob[ (X- \expect(X)) \geq a] \leq \frac{\vari(X)}{a^2} \]
\end{cor}

\begin{proof}
  Let $Y \coloneqq (X - \expect(X))^2 \geq0$.
  \begin{align*}
    \prob[ (X- \expect(X)) \geq a] &= \prob[(X-\expect(X))^2 \geq a] \\
    &= \prob[Y \geq \frac{a^2 \expect(Y)}{\expect(Y)}] \\
    &= \prob[Y \geq \frac{a^2}{\expect(Y)} \expect(Y)] \\
    &= \prob[Y \geq \frac{a^2}{\tilde{a}} \expect(Y)] \\
    &\geq \frac{\expect(Y)}{a^2} \\
    &= \frac{\vari(X)}{a^2}
  \end{align*}
\end{proof}

\begin{theorem}[weak law of large numbers]
  Let $(X_n)_{n \in \N}$ be a sequence of iid (independent identically distributed) random variables with $\mu = \expect(X_n)$ and $\sigma^2 = \vari(X_n)$ finite and $\sigma^2 > 0$.
  \[ \overbar{X_n} = \frac{1}{n}(X_1 + \dots + X_n)\]
  Then
  \[ \overbar{X_n} \to \mu \]
  in probability.
\end{theorem}

\begin{proof}
  $\expect(\overbar{X_n}) = \mu$
  \begin{align*}
    \vari(\overbar{X_n}) &= \frac{1}{n^2} \vari(X_1 +\dots + X_n) \\
    &= \frac{1}{n^2} (\vari(X_1) +\dots + \vari(X_n)) \\
    &= \frac{\sigma^2}{n}
  \end{align*}
  \begin{align*}
    \prob[\abs{\overbar{X_n}-\mu} &\geq a] \leq \frac{\vari(\overbar{X_n})}{a^2} \\
    &= \frac{\sigma^2}{n a^2} \to 0
  \end{align*}
  for all $a$.
\end{proof}

\end{document}
